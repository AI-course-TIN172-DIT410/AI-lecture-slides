<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <style type="text/css">
    /*pre { color:#c7254e; background-color:#f9f2f4; }*/
    code { color:#333; background-color: #f5f5f5; }
    .pl-c{color:#969896}
    .pl-c1,.pl-s .pl-v{color:#0086b3}
    .pl-e,.pl-en{color:#795da3}
    .pl-s .pl-s1,.pl-smi{color:#333}
    .pl-ent{color:#63a35c}
    .pl-k{color:#a71d5d}
    .pl-pds,.pl-s,.pl-s .pl-pse .pl-s1,.pl-sr,.pl-sr .pl-cce,.pl-sr .pl-sra,.pl-sr .pl-sre{color:#183691}
    .pl-v{color:#ed6a43}
    .pl-id{color:#b52a1d}
    .pl-ii{background-color:#b52a1d;color:#f8f8f8}
    .pl-sr .pl-cce{color:#63a35c;font-weight:bold}
    .pl-ml{color:#693a17}
    .pl-mh,.pl-mh .pl-en,.pl-ms{color:#1d3e81;font-weight:bold}
    .pl-mq{color:#008080}
    .pl-mi{color:#333;font-style:italic}
    .pl-mb{color:#333;font-weight:bold}
    .pl-md{background-color:#ffecec;color:#bd2c00}
    .pl-mi1{background-color:#eaffea;color:#55a532}
    .pl-mdr{color:#795da3;font-weight:bold}
    .pl-mo{color:#1d3e81}
  </style>
</head>

<body role="document">
<div class="container">
<h1>
<a id="user-content-natural-language-processing--interpretation" class="anchor" href="#natural-language-processing--interpretation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Natural Language Processing &amp; Interpretation</h1>

<p>TIN173/DIT410 Artificial Intelligence<br>
John J. Camilleri<br>
2017-04-04</p>

<h1>
<a id="user-content-what-is-natural-language" class="anchor" href="#what-is-natural-language" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What is natural language?</h1>

<p><strong>Natural language</strong></p>

<ul>
<li>Any language that develops naturally in humans through use and repetition</li>
<li>e.g.: English, Swedish</li>
<li>Imprecisely defined: is <code>"I totally lol'ed"</code> correct?</li>
<li>Interpretation can be ambiguous</li>
</ul>

<p><strong>Formal language</strong></p>

<ul>
<li>Specifically constructed for some purpose</li>
<li>e.g.: JavaScript, propositional logic</li>
<li>Precisely defined

<ul>
<li>
<code>print(1 + 2)</code> <g-emoji alias="white_check_mark" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/2705.png" ios-version="6.0">✅</g-emoji>
</li>
<li>
<code>print(+ 1 2.</code> <g-emoji alias="x" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/274c.png" ios-version="6.0">❌</g-emoji>
</li>
</ul>
</li>
<li>Unambiguous, precise semantics</li>
</ul>

<h1>
<a id="user-content-natural-language-processing-nlp" class="anchor" href="#natural-language-processing-nlp" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Natural Language Processing (NLP)</h1>

<ul>
<li>Computational methods to natural language</li>
<li>Well-defined tasks, often objectively evaluatable</li>
<li>Different modalities: text / speech</li>
<li>Different directions: analysis / generation</li>
</ul>

<h2>
<a id="user-content-examples-of-nlp-tasks" class="anchor" href="#examples-of-nlp-tasks" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Examples of NLP tasks</h2>

<ul>
<li>Classification, e.g. sentiment analysis</li>
<li>Information extraction, e.g. Named entity recognition</li>
<li>Information retrieval (search)

<ul>
<li>corpus of documents</li>
<li>query (in given language)</li>
<li>result set</li>
</ul>
</li>
<li>Machine translation, e.g. Google Translate</li>
</ul>

<h2>
<a id="user-content-rules-vs-statistics" class="anchor" href="#rules-vs-statistics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules vs Statistics</h2>

<ul>
<li>These days many NLP tasks are powered by machine learning</li>
<li>We have lots of data (corpora), fast processors, cheap storage</li>
<li>Up to the 1980s, most NLP systems were based on complex sets of hand-written rules</li>
<li>Linguistics itself is quite a formal discipline</li>
</ul>

<h1>
<a id="user-content-phrase-structure-grammars" class="anchor" href="#phrase-structure-grammars" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Phrase-Structure Grammars</h1>

<ul>
<li>Words have different lexical categories: noun, verb, adjective...</li>
<li>Combine into phrasal categories:<br>
<code>the</code> (determiner) + <code>house</code> (noun) = <code>the house</code> (noun phrase)</li>
<li>Grammar is a set of rules describes which combinations are possible</li>
<li>Goal: describe the set of sentences which are correct in a language<br>
and reject those which are invalid

<ul>
<li>
<code>the man saw a mountain</code> <g-emoji alias="white_check_mark" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/2705.png" ios-version="6.0">✅</g-emoji>
</li>
<li>
<code>the man saw a</code> <g-emoji alias="x" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/274c.png" ios-version="6.0">❌</g-emoji>
</li>
</ul>
</li>
<li>Assign <em>parse trees</em> to valid sentences</li>
</ul>

<h1>
<a id="user-content-context-free-grammars" class="anchor" href="#context-free-grammars" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Context-free grammars</h1>

<p>A context-free grammar is a 4-tuple <strong>G = (V, Σ, R, S)</strong> where:</p>

<ul>
<li>V is a finite set of non-terminals (called "syntactic categories")</li>
<li>Σ is a finite set of terminals (disjoint from V)</li>
<li>R is the set of production rules, a finite relation from V to (V ⊔ Σ)* , e.g.:

<ul>
<li><code>X → Y</code></li>
<li><code>X → a</code></li>
<li><code>X → b Z</code></li>
</ul>
</li>
<li>S is the start symbol ∈ V</li>
</ul>

<p>The <em>language</em> <code>L</code> of a grammar <code>G</code> is the set of strings which have a valid derivation from <code>S</code>:</p>

<pre><code>L(G) = { w ∈ Σ* : S ⇒* w}
</code></pre>

<h2>
<a id="user-content-example-grammar" class="anchor" href="#example-grammar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Example grammar</h2>

<p><strong>S</strong> = sentence,
<strong>NP</strong> = noun phrase,
<strong>VP</strong> = verb phrase,
<strong>Det</strong> = determiner,
<strong>N</strong> = noun,
<strong>V</strong> = verb</p>

<h4>
<a id="user-content-rules" class="anchor" href="#rules" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S  → NP VP
NP → Det N
VP → V NP
</code></pre>

<h4>
<a id="user-content-lexicon" class="anchor" href="#lexicon" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>N   → man | mountain
V   → saw
Det → a | the
</code></pre>

<blockquote>
<p>"the man saw a mountain" <g-emoji alias="white_check_mark" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/2705.png" ios-version="6.0">✅</g-emoji>  </p>
</blockquote>

<p><a href="tree-1.png" target="_blank"><img src="img/nlp/tree-1.png" alt="the man saw a mountain" style="max-width:100%;"></a></p>

<blockquote>
<p>"the man saw a" <g-emoji alias="x" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/274c.png" ios-version="6.0">❌</g-emoji>   </p>
</blockquote>

<p><a href="tree-NP-the_man.png" target="_blank"><img src="img/nlp/tree-NP-the_man.png" alt="the man" style="max-width:100%;"></a>
<a href="tree-V-saw.png" target="_blank"><img src="img/nlp/tree-V-saw.png" alt="saw" style="max-width:100%;"></a>
<a href="tree-Det-a.png" target="_blank"><img src="img/nlp/tree-Det-a.png" alt="a" style="max-width:100%;"></a><br>
<a href="tree-2.png" target="_blank"><img src="img/nlp/tree-2.png" alt="the man saw a ?" style="max-width:100%;"></a>
No complete tree starting wih <code>S</code>!</p>

<h2>
<a id="user-content-bigger-example" class="anchor" href="#bigger-example" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bigger example</h2>

<p>We add <em>prepositions</em> <strong>Prep</strong> ("with", "on") and
<em>prepositional phrases</em> <strong>PP</strong>.</p>

<h4>
<a id="user-content-rules-1" class="anchor" href="#rules-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S  → NP VP
NP → Det N
NP → NP PP
VP → V NP
VP → VP PP
PP → Prep NP
</code></pre>

<h4>
<a id="user-content-lexicon-1" class="anchor" href="#lexicon-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>NP   → Mary
N    → man | mountain | telescope
V    → saw
Det  → a | the
Prep → with | on
</code></pre>

<blockquote>
<p>"Mary saw the man on the mountain with a telescope"</p>
</blockquote>

<ul>
<li>The meaning of this sentence is <strong>ambiguous</strong>!</li>
<li>The problem is PP-attachment</li>
<li>The prepositional phrase can be attached to both NPs and VPs</li>
<li>The parse trees show us how the interpretations differ</li>
<li>All are equally valid syntactically</li>
</ul>

<p><strong>"saw ... with a telescope"</strong><br>
<a href="telescope-1.png" target="_blank"><img src="img/nlp/telescope-1.png" alt="" style="max-width:100%;"></a>
<a href="telescope-2.png" target="_blank"><img src="img/nlp/telescope-2.png" alt="" style="max-width:100%;"></a></p>

<p><strong>"... on the mountain with a telescope"</strong><br>
<a href="telescope-3.png" target="_blank"><img src="img/nlp/telescope-3.png" alt="" style="max-width:100%;"></a>
<a href="telescope-4.png" target="_blank"><img src="img/nlp/telescope-4.png" alt="" style="max-width:100%;"></a></p>

<p><strong>"the man ... with a telescope"</strong><br>
<a href="telescope-5.png" target="_blank"><img src="img/nlp/telescope-5.png" alt="" style="max-width:100%;"></a>
<a href="telescope-men.jpg" target="_blank"><img src="img/nlp/telescope-men.jpg" alt="" style="max-width:100%;"></a></p>

<h1>
<a id="user-content-syntactic-analysis-parsing" class="anchor" href="#syntactic-analysis-parsing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Syntactic analysis (parsing)</h1>

<ul>
<li>Given a grammar, find a derivation from <code>S</code> for an input string</li>
<li>Function from string to a list of parse trees<br>
<code>parse(g : Grammar, s : String) : ParseTree[]</code>

<ul>
<li>0 trees: input is <strong>invalid</strong>
</li>
<li>1 tree: input is <strong>valid</strong> and <strong>unambiguous</strong>
</li>
<li>2+ trees: input is <strong>valid</strong> and <strong>ambiguous</strong>
</li>
</ul>
</li>
</ul>

<h2>
<a id="user-content-algorithms" class="anchor" href="#algorithms" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Algorithms</h2>

<ul>
<li>Parsing is a search problem</li>
<li>Top-down

<ul>
<li>Start from <code>S</code> and apply rules until you match input</li>
<li>Wastes time building subtrees that will never match input</li>
</ul>
</li>
<li>Bottom-up

<ul>
<li>Apply rules backwards based on input</li>
<li>Wastes time building subtrees which will never join at <code>S</code>
</li>
</ul>
</li>
<li>Chart parsing

<ul>
<li>Use dynamic programming to store derivations for sub-strings</li>
<li>Improve efficiency of algorithm by avoiding <em>backtracking</em>
</li>
</ul>
</li>
</ul>

<h3>
<a id="user-content-parsing-demos-in-python" class="anchor" href="#parsing-demos-in-python" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parsing demos in Python</h3>

<ul>
<li>Top-down (pecursive-descent): <code>$ python -m nltk.app.rdparser_app</code>
</li>
<li>Bottom-up (shift-reduce): <code>$ python -m nltk.app.rdparser_app</code>
</li>
</ul>

<h2>
<a id="user-content-cyk-algorithm" class="anchor" href="#cyk-algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>CYK algorithm</h2>

<ul>
<li>Bottom-up chart parser</li>
<li>Example of a parsing "chart":</li>
</ul>

<table class="table">
<thead>
<tr>
<th align="left">Mary</th>
<th align="left">saw</th>
<th align="left">the</th>
<th align="left">man</th>
<th align="left">with</th>
<th align="left">a</th>
<th align="left">telescope</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">[0,1] NP</td>
<td align="left">[0,2] -</td>
<td align="left">[0,3] -</td>
<td align="left">[0,4] S</td>
<td align="left">[0,5] -</td>
<td align="left">[0,6] -</td>
<td align="left">[0,7] S¹, S²</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">[1,2] V</td>
<td align="left">[1,3] -</td>
<td align="left">[1,4] VP</td>
<td align="left">[1,5] -</td>
<td align="left">[1,6] -</td>
<td align="left">[1,7] VP¹, VP²</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">[2,3] Det</td>
<td align="left">[2,4] NP</td>
<td align="left">[2,5] -</td>
<td align="left">[2,6] -</td>
<td align="left">[2,7] NP</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">[3,4] N</td>
<td align="left">[3,5] -</td>
<td align="left">[3,6] -</td>
<td align="left">[3,7] -</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">[4,5] Prep</td>
<td align="left">[4,6] -</td>
<td align="left">[4,7] PP</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">[5,6] Det</td>
<td align="left">[5,7] NP</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">[6,7] N</td>
</tr>
</tbody>
</table>

<p>Note: at [1,7] there are two ways of building a VP, introducing ambiguity.</p>

<p>S¹: <a href="mary-saw-the-man-with-a-telescope-1.png" target="_blank"><img src="img/nlp/mary-saw-the-man-with-a-telescope-1.png" alt="" style="max-width:100%;"></a><br>
<code>(S (NP Mary) (VP (V saw) (NP (NP (Det the) (N man)) (PP (Prep with) (NP (Det a) (N telescope))))))</code></p>

<p>S²: <a href="mary-saw-the-man-with-a-telescope-2.png" target="_blank"><img src="img/nlp/mary-saw-the-man-with-a-telescope-2.png" alt="" style="max-width:100%;"></a><br>
<code>(S (NP Mary) (VP (VP (V saw) (NP (Det the) (N man))) (PP (Prep with) (NP (Det a) (N telescope)))))</code></p>

<ul>
<li>Space: <strong>O(n²m)</strong>, Time: <strong>O(n³m)</strong><br>
n = number of input symbols<br>
m = number non-terminals in grammar (constant for grammar)</li>
<li>Number of parse trees is actually exponential! O(2ⁿ)</li>
<li>Another well-known parsing algorithm: <strong>Earley parser</strong>
</li>
</ul>

<h1>
<a id="user-content-probabilitistic-parsing" class="anchor" href="#probabilitistic-parsing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Probabilitistic parsing</h1>

<ul>
<li>How to choose amongst multiple ambiguous parses?</li>
<li>Assign probability values to rules and rank parses by combined probability</li>
<li>Probabilites can be learned from an annotated corpus (treebank)</li>
</ul>

<h2>
<a id="user-content-pcfg-example" class="anchor" href="#pcfg-example" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PCFG example</h2>

<h4>
<a id="user-content-rules-2" class="anchor" href="#rules-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S  → NP VP    [1.0]
NP → Det N    [0.5]
NP → NP PP    [0.4]
VP → V NP     [0.8]
VP → VP PP    [0.2]
PP → Prep NP  [1.0]
</code></pre>

<h4>
<a id="user-content-lexicon-2" class="anchor" href="#lexicon-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>NP   → Mary [0.1]
N    → man  [0.6] | mountain [0.2] | telescope [0.2]
V    → saw  [1.0]
Det  → a    [0.4] | the [0.6]
Prep → with [0.5] | on  [0.5]
</code></pre>

<h4>
<a id="user-content-parse-tree-probability" class="anchor" href="#parse-tree-probability" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Parse tree probability</h4>

<p><a href="telescope-1.png" target="_blank"><img src="img/nlp/telescope-1.png" alt="" style="max-width:100%;"></a><br>
1.0 × 0.1 × 0.2 × 0.8 × 1.0 × 0.4 × 0.5 × 0.6 × 0.6 × 1.0 × 0.5 × 0.5 × 0.6 × 0.2 × 1.0 × 0.5 × 0.5 × 0.4 × 0.2 = <strong>0.0000006912</strong> = <strong>6.912 e-7</strong></p>

<p><a href="telescope-3.png" target="_blank"><img src="img/nlp/telescope-3.png" alt="" style="max-width:100%;"></a><br>
1.0 × 0.1 × 0.8 × 1.0 × 0.4 × 0.5 × 0.6 × 0.6 × 1.0 × 0.5 × 0.4 × 0.5 × 0.6 × 0.2 × 1.0 × 0.5 × 0.5 × 0.4 × 0.2 = <strong>0.0000013824</strong> = <strong>1.3824 e-6</strong></p>

<ul>
<li>Thus the second tree is more likely and should be ranked first.</li>
<li>These values must be <em>smoothed</em> so that an out-of-corpus phrase does not have 0 probability.</li>
</ul>

<h1>
<a id="user-content-overgeneration" class="anchor" href="#overgeneration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overgeneration</h1>

<p>Let's add the pronoun <code>I</code> and present tense verb <code>see</code>:</p>

<pre><code>NP   → Mary | I
V    → saw  | see
</code></pre>

<blockquote>
<p><code>I see Mary</code> <g-emoji alias="white_check_mark" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/2705.png" ios-version="6.0">✅</g-emoji></p>
</blockquote>

<p><a href="i-see-mary.png" target="_blank"><img src="img/nlp/i-see-mary.png" alt="" style="max-width:100%;"></a></p>

<blockquote>
<p><code>Mary saw I</code> <g-emoji alias="x" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/274c.png" ios-version="6.0">❌</g-emoji></p>
</blockquote>

<p><a href="mary-saw-i.png" target="_blank"><img src="img/nlp/mary-saw-i.png" alt="" style="max-width:100%;"></a>  </p>

<ul>
<li>Pronoun case agreement</li>
<li>
<code>I</code> in the subject position (<em>nominative</em>) but <code>me</code> in the object position (<em>accusative</em>)</li>
</ul>

<blockquote>
<p><code>Mary see a telescope</code> <g-emoji alias="x" fallback-src="https://assets-cdn.github.com/images/icons/emoji/unicode/274c.png" ios-version="6.0">❌</g-emoji></p>
</blockquote>

<p><a href="mary-see-a-telescope.png" target="_blank"><img src="img/nlp/mary-see-a-telescope.png" alt="" style="max-width:100%;"></a>  </p>

<ul>
<li>Subject-verb agreement</li>
<li>
<code>I see</code> (first person) but <code>Mary sees</code> (third person)</li>
<li>These problems are <strong>much worse</strong> in other languages!<br>
English grammar is relatively simple.</li>
</ul>

<h1>
<a id="user-content-solution-1-adding-more-rules" class="anchor" href="#solution-1-adding-more-rules" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solution 1: adding more rules</h1>

<h3>
<a id="user-content-11-handling-pronoun-case" class="anchor" href="#11-handling-pronoun-case" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.1: Handling pronoun case</h3>

<p>Separate rules for noun-phrases in <em>subject</em> position (<code>NP_S</code>)
and noun phrases in <em>object</em> position (<code>NP_O</code>).</p>

<h4>
<a id="user-content-rules-3" class="anchor" href="#rules-3" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S    → NP_S VP
NP_S → Det N
NP_S → NP_S PP
NP_O → Det N
NP_O → NP_O PP
VP   → V NP_O
VP   → VP PP
PP   → Prep NP_S
PP   → Prep NP_O
</code></pre>

<h4>
<a id="user-content-lexicon-3" class="anchor" href="#lexicon-3" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>NP_S → Mary | I
NP_O → Mary | me
N    → man  | mountain | telescope
V    → saw  | see
Det  → a    | the
Prep → with | on
</code></pre>

<h3>
<a id="user-content-12-handling-verb-subject-agreement" class="anchor" href="#12-handling-verb-subject-agreement" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1.2: Handling verb-subject agreement</h3>

<p>Separate rules for noun-phrases in the <em>first-person</em> (<code>NP_?_P1</code>)
and noun phrases the <em>third-person</em> (<code>NP_?_P3</code>).</p>

<h4>
<a id="user-content-rules-4" class="anchor" href="#rules-4" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S       → NP_S_P1 VP_P1
S       → NP_S_P3 VP_P3
NP_S_P1 → Det N
NP_S_P1 → NP_S_P1 PP
NP_O_P1 → Det N
NP_O_P1 → NP_O_P1 PP
NP_S_P3 → Det N
NP_S_P3 → NP_S_P3 PP
NP_O_P3 → Det N
NP_O_P3 → NP_O_P3 PP
VP_P1   → V_P1 NP_O_P1
VP_P1   → V_P1 NP_O_P3
VP_P1   → VP_P1 PP
VP_P3   → V_P3 NP_O_P1
VP_P3   → V_P3 NP_O_P3
VP_P3   → VP_P3 PP
PP      → Prep NP_S_P1
PP      → Prep NP_O_P1
PP      → Prep NP_S_P3
PP      → Prep NP_O_P3
</code></pre>

<h4>
<a id="user-content-lexicon-4" class="anchor" href="#lexicon-4" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>NP_S_P1 → I
NP_O_P1 → me
NP_S_P3 → Mary
NP_O_P3 → Mary
N       → man  | mountain | telescope
V_P1    → saw  | see
V_P3    → saw  | sees
Det     → a    | the
Prep    → with | on
</code></pre>

<ul>
<li>This becomes messy!</li>
<li>A program can generate these rules for us.</li>
<li>But remember the parsing complexity depends on the size of the grammar! <strong>O(n³m)</strong>
</li>
<li>Explosion of number of rules is still bad, even if complexity is polynomial.</li>
</ul>

<h1>
<a id="user-content-solution-2-augmenting-the-grammar" class="anchor" href="#solution-2-augmenting-the-grammar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Solution 2: augmenting the grammar</h1>

<ul>
<li>Definite clause grammar (DCG)</li>
<li>Add parameters to the rules</li>
<li>Use constraints to limit the over-generation</li>
<li>Takes us beyond the expressove power of CFG</li>
</ul>

<h4>
<a id="user-content-rules-5" class="anchor" href="#rules-5" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Rules</h4>

<pre><code>S        → NP(n,c) VP(n) {c = Sbj}
NP(P3,_) → Det N
NP(n,c)  → NP(n,c) PP
VP(n)    → V(n) NP(_,c) {c = Obj}
VP(n)    → VP(n) PP
PP       → Prep NP(_,_)
</code></pre>

<h4>
<a id="user-content-lexicon-5" class="anchor" href="#lexicon-5" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Lexicon</h4>

<pre><code>NP(P3,_)   → Mary
NP(P1,Sbj) → I
NP(P1,Obj) → me
N     → man | mountain | telescope
V(P1) → saw | see
V(P3) → saw | sees
Det   → a | the
Prep  → with | on
</code></pre>

<h1>
<a id="user-content-generative-capacity-chomsky-hierarchy" class="anchor" href="#generative-capacity-chomsky-hierarchy" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generative capacity (Chomsky hierarchy)</h1>

<ul>
<li>Try to build a regular expression which matches an equal number of a's and b's: <code>{ aⁿbⁿ, n &gt; 0 }</code>.<br>
This is theoretically impossible to do!</li>
<li>This is why you can't write a regular expression to find matching HTML tags, for example.</li>
<li>This can easily be captured in a CFG, however: <code>S → a S b | ϵ</code>
</li>
<li><p>But now try to build a CFG which matches an equal number of a's, b's and c's: <code>{ aⁿbⁿcⁿ, n &gt; 0 }</code>.<br>
Again, we find this is theoretically impossible to do, since that goes beyong the <strong>expressive power</strong> of CFGs.</p></li>
<li><p>Context-free grammars are popular because parsing them is quite efficient</p></li>
<li>All programming languages' syntax are basically CF</li>
<li>But CFGs have been proven not to be sufficient for describing all of natural language.</li>
</ul>

<table class="table">
<thead>
<tr>
<th align="left">Grammar</th>
<th align="left">Languages</th>
<th align="left">Automaton</th>
<th align="left">Production rules (constraints)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Type-0</td>
<td align="left">Recursively enumerable</td>
<td align="left">Turing machine</td>
<td align="left">α → β (no restrictions)</td>
</tr>
<tr>
<td align="left">Type-1</td>
<td align="left">Context-sensitive</td>
<td align="left">Linear-bounded non-deterministic Turing machine</td>
<td align="left">α A β → α γ β</td>
</tr>
<tr>
<td align="left">Type-2</td>
<td align="left">Context-free</td>
<td align="left">Non-deterministic pushdown automaton</td>
<td align="left">A → γ</td>
</tr>
<tr>
<td align="left">Type-3</td>
<td align="left">Regular</td>
<td align="left">Finite state automaton</td>
<td align="left">A → a and A → aB</td>
</tr>
</tbody>
</table>

<p><em>where</em></p>

<ul>
<li>
<strong>A, B</strong> are non-terminals</li>
<li>
<strong>α, β, γ</strong> are strings of terminals/non-terminals</li>
</ul>

<h1>
<a id="user-content-more-on-ambiguity" class="anchor" href="#more-on-ambiguity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>More on ambiguity</h1>

<p>Most of the sentences we hear seem unambiguous.<br>
But almost every utterance contains some kinds of ambiguity.<br>
We are just very good at <strong>disambiguating</strong>!</p>

<p><strong>Levels of ambiguity</strong></p>

<ul>
<li>Lexical

<ul>
<li>A word can belong to multiple categories</li>
<li><em>Buffalo buffalo buffalo buffalo</em></li>
<li>
<strong>noun</strong> (animal, American bison)</li>
<li>
<strong>verb</strong> (to confuse)</li>
<li>
<strong>adjective</strong> (coming from Buffalo, NY)</li>
<li><em>"Bison [from] Buffalo [often] confuse [other] bison"</em></li>
</ul>
</li>
<li>Syntactic

<ul>
<li>Phrases can attach at different points in the tree</li>
<li><em>I ordered a pizza with rucola</em></li>
<li><em>I ordered a pizza with my phone</em></li>
</ul>
</li>
<li>Semantic

<ul>
<li>Multiple interprations</li>
<li>Common result of syntactic ambiguity</li>
<li>
<em>Everybody loves somebody</em><br>
<code>∀x ∃y · Loves(x,y)</code> or <code>∃y ∀x · Loves(x,y)</code> ?</li>
</ul>
</li>
</ul>

<h1>
<a id="user-content-disambiguation" class="anchor" href="#disambiguation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Disambiguation</h1>

<p>Probabilites as in PCFG only help us choose most likely phrase
(most common in a corpus).
But that is not taking <strong>context</strong> into account!</p>

<p>We need models to disambiguate.</p>

<h4>
<a id="user-content-acoustic-model" class="anchor" href="#acoustic-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acoustic model</h4>

<ul>
<li>When dealing with speech.</li>
<li>The likelihood that a particular sequence of sounds occurs,
given a string of words.</li>
</ul>

<h4>
<a id="user-content-language-model" class="anchor" href="#language-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Language model</h4>

<ul>
<li>The likelihood that a certain string of words appear together

<ul>
<li>
<em>pizza with rucola</em> (quite likely)</li>
<li>
<em>pizza with friends</em> (somewhat likely)</li>
<li>
<em>pizza with monkeys</em> (quite unlikely)</li>
</ul>
</li>
</ul>

<h4>
<a id="user-content-mental-model" class="anchor" href="#mental-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mental model</h4>

<ul>
<li>What the speaker believes</li>
<li>What the speaker believes the hearer believes, etc.</li>
</ul>

<p>Politician saying: <em>I am not a crook</em></p>

<ul>
<li>crook as in a liar</li>
<li>crook as in a hooked staff of a shepherd</li>
</ul>

<h4>
<a id="user-content-world-model" class="anchor" href="#world-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>World model</h4>

<ul>
<li>The likelihood that a proposition holds in the world</li>
<li>Contains knowledge about the world:

<ul>
<li><em>rucola is a pizza topping</em></li>
<li><em>a phone can be used to order something</em></li>
</ul>
</li>
</ul>
</div> <!-- container -->
</body>
</html>
